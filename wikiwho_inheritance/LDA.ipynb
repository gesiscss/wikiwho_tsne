{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import CoherenceModel\n",
    "from nltk.corpus import stopwords\n",
    "stop = set(stopwords.words('english'))\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  \n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('./39570/vectorizers/Bert_LRG___Chobs_context_5_gap_length_20.pkl')\n",
    "df_normal = pd.read_pickle('./39570/clusterers/Bert_LRG___Chobs_context_5_gap_length_20/tsne/DBscan_min_samples_4_eps_1.5.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cut = df[['left_token_str_clean', 'ins_tokens_str_clean', 'right_token_str_clean']]\n",
    "corpus = []\n",
    "for i, row in df_cut.iterrows():\n",
    "    one_sent = row[0] + ' ' + row[1] + ' ' + row[2]\n",
    "    corpus.append([i for i in one_sent.split() if i not in stop])\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all tokens (except for stopwords)\n",
    "dictionary = corpora.Dictionary(corpus)\n",
    "doc_term_matrix = [dictionary.doc2bow(sent) for sent in corpus]\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=100, id2word = dictionary, passes=50)\n",
    "pprint(ldamodel.print_topics(num_topics=10, num_words=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0 #total number of tokens\n",
    "for sent in corpus:\n",
    "    counter += len(sent)\n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deleting those tokens that appear only once\n",
    "all_tokens = sum(corpus, [])\n",
    "tokens_once = set(word for word in set(all_tokens) if all_tokens.count(word) == 1)\n",
    "sents = [[word for word in text if word not in tokens_once]\n",
    "         for text in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0 #number of tokens that appear more than once\n",
    "for sent in sents:\n",
    "    counter += len(sent)\n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(sents)\n",
    "doc_term_matrix = [dictionary.doc2bow(sent) for sent in sents]\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=100, id2word = dictionary, passes=50)\n",
    "pprint(ldamodel.print_topics(num_topics=10, num_words=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(sents)\n",
    "corpus = [dictionary.doc2bow(text) for text in sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=dictionary,\n",
    "                                           num_topics=80, \n",
    "                                           #random_state=100,\n",
    "                                           #update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(lda_model.print_topics(num_topics=10, num_words=10))\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=sents, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary, mds='tsne')\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(dictionary, corpus, texts, limit, start, step): #DOES NOT WORK, ISSUE WITH GENSIM\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, passes=10, chunksize=100)\n",
    "        print(model)\n",
    "        model_list.append(model)\n",
    "        coherence_model_lda = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_lda = coherence_model_lda.get_coherence()\n",
    "        print(coherence_lda)\n",
    "        coherence_values.append(coherence_lda)\n",
    "\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_list, coherence_values = compute_coherence_values(dictionary=dictionary, corpus=corpus, texts=sents, start=60, limit=100, step=5)\n",
    "#coherence_values = []\n",
    "model = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=100, passes=10, chunksize=100) #MANUALLY CHANGE NUM_TOPICS\n",
    "coherence_model_lda = CoherenceModel(model=model, texts=sents, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "coherence_values.append(coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list, coherence_values = compute_coherence_values(dictionary=dictionary, corpus=corpus, texts=sents, start=20, limit=100, step=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start=20; limit=100; step=10\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list, coherence_values = compute_coherence_values(dictionary=dictionary, corpus=corpus, texts=sents, start=30, limit=40, step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_updated = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=dictionary,\n",
    "                                           num_topics=37, \n",
    "                                           #random_state=100,\n",
    "                                           #update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10)\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model_updated, corpus, dictionary, mds='tsne')\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "lda_token = widgets.Text(\n",
    "    value=None,\n",
    "    description = 'Token:',\n",
    "    continuous_update = False\n",
    ")\n",
    "\n",
    "def w_on_change(change):\n",
    "    if change['type'] == 'change' and change['name'] == 'value':\n",
    "        print('For ', lda_token.value, ': ', lda_model_updated.get_term_topics(dictionary.doc2idx([lda_token.value]), minimum_probability=None))\n",
    "\n",
    "\n",
    "lda_token.observe(w_on_change)\n",
    "display(lda_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lda_model_updated.print_topic(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2bow_func = lambda x: dictionary.doc2bow(x) \n",
    "doc_term_matrix = list(map(doc2bow_func, sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_topics = lambda x: lda_model_updated.get_document_topics(x, minimum_probability=0)\n",
    "result = list(map(get_topics, doc_term_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_to_chobj = []\n",
    "for chobj in result:\n",
    "    topic_to_chobj.append(max(chobj,key=lambda item:item[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_ids(sents):\n",
    "    token_to_ids = []\n",
    "    for obj in sents:\n",
    "        id_unique = dictionary.doc2idx(obj)\n",
    "        token_to_ids.append(id_unique)\n",
    "    return token_to_ids\n",
    "\n",
    "def get_topics(token_to_ids):\n",
    "    topics_all_chobj = []\n",
    "    one_topic_dict = []\n",
    "    for chobj in token_to_ids:\n",
    "        topic_dict = {}\n",
    "        \n",
    "        counter_max = 0\n",
    "        for token_id in chobj:\n",
    "            topic_pr = lda_model_updated.get_term_topics(token_id, minimum_probability=None)\n",
    "            if len(topic_pr) == 1 and len(topic_pr[0])>0: #when a word has only one topic assigned to it and it is not null\n",
    "                if str(topic_pr[0][0]) not in topic_dict.keys(): # if a topic is encountered for the first time\n",
    "                    topic_dict[str(topic_pr[0][0])] = (topic_pr[0][1], 1)\n",
    "                    if counter_max == 0:\n",
    "                        counter_max = 1\n",
    "                        topic_max = topic_pr[0][0]\n",
    "                        prob_max = topic_pr[0][1]\n",
    "                elif str(topic_pr[0][0]) in topic_dict.keys(): #if a topic has already been encountered \n",
    "                    pr_counter = (topic_dict[str(topic_pr[0][0])][0] + topic_pr[0][1], topic_dict[str(topic_pr[0][0])][1] + 1)\n",
    "                    topic_dict[str(topic_pr[0][0])] = pr_counter\n",
    "                    if pr_counter[1] > counter_max :\n",
    "                        counter_max = pr_counter[1]\n",
    "                        topic_max = str(topic_pr[0][0])\n",
    "                        prob_max = pr_counter[0]\n",
    "                    elif pr_counter[1] == counter_max :\n",
    "                        if pr_counter[0] > prob_max:\n",
    "                            prob_max = pr_counter[0]\n",
    "                            topic_max = str(topic_pr[0][0])\n",
    "            elif len(topic_pr) > 1: #when a word has more than one topic assigned to it\n",
    "                pr_max = topic_pr[0][1]\n",
    "                for i in range(1,len(topic_pr)): #to choose the topic with max probability, MAYBE NEEDS OTHER METHOD LIKE ADDING ALL TOPICS\n",
    "                    if topic_pr[i][1] > pr_max:\n",
    "                        tuple_max = topic_pr[i]\n",
    "                if str(tuple_max[0]) not in topic_dict.keys():\n",
    "                    topic_dict[str(tuple_max[0])] = (tuple_max[1], 1)\n",
    "                    counter_max = 1\n",
    "                    topic_max = str(tuple_max[0])\n",
    "                    prob_max = tuple_max[1]\n",
    "                elif str(tuple_max[0]) in topic_dict.keys():\n",
    "                    pr_counter = (topic_dict[str(tuple_max[0])][0] + tuple_max[1], topic_dict[str(tuple_max[0])][1] + 1)\n",
    "                    if pr_counter[1] > counter_max :\n",
    "                        counter_max = pr_counter[1]\n",
    "                        topic_max = str(tuple_max[0])\n",
    "                        prob_max = pr_counter[0]\n",
    "                    elif pr_counter[1] == counter_max :\n",
    "                        if pr_counter[0] > prob_max:\n",
    "                            prob_max = pr_counter[0]\n",
    "                            topic_max = str(tuple_max[0])\n",
    "                    topic_dict[str(tuple_max[0])] = pr_counter\n",
    "\n",
    "        one_topic_dict.append((topic_max, prob_max))\n",
    "        topics_all_chobj.append(topic_dict)\n",
    "    return topics_all_chobj, one_topic_dict\n",
    "\n",
    "token_to_ids = get_token_ids(sents)\n",
    "topics_all_chobj, one_topic_by_occur = get_topics(token_to_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_norm_probabilities(topics_all_chobj):\n",
    "# a method for normalizing probabilities (*number of occurences of a topic in chobj / total number of occurences of all topics in chobj)\n",
    "    norm_topics_all_chobj = [] \n",
    "    for chobj in topics_all_chobj:\n",
    "        chobj_counter = 0\n",
    "        for k, v in chobj.items():\n",
    "            chobj_counter += v[1]\n",
    "        for k in chobj.keys():\n",
    "            norm_prob = chobj[k][0] * chobj[k][1] / chobj_counter \n",
    "            chobj[k] = norm_prob\n",
    "        norm_topics_all_chobj.append(chobj)\n",
    "    return norm_topics_all_chobj\n",
    "\n",
    "def get_one_topic_by_prob(topics_all_chobj):\n",
    "# a method for choosing a topic with the highest probability\n",
    "    one_topic_all_chobs = []\n",
    "    for chobj in norm_topics_all_chobj:\n",
    "        if len(chobj) > 1:\n",
    "            topic_max = max(chobj, key=chobj.get)\n",
    "            pair_max = (topic_max, chobj[topic_max])\n",
    "        elif len(chobj) == 1:\n",
    "            pair_max = next(iter( chobj.items() ))\n",
    "        one_topic_all_chobs.append(pair_max)\n",
    "    \n",
    "    return one_topic_all_chobs\n",
    "\n",
    "norm_topics_all_chobj = get_norm_probabilities(topics_all_chobj)\n",
    "one_topic_by_prob = get_one_topic_by_prob(norm_topics_all_chobj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clusters'] = [int(i[0]) for i in topic_to_chobj]\n",
    "df['t-SNE-X'] = df_normal['t-SNE-X']\n",
    "df['t-SNE-Y'] = df_normal['t-SNE-Y']\n",
    "max(df['clusters'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "data = df['clusters'].value_counts()\n",
    "data.plot.bar()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.io\n",
    "import plotly\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "import plotly.graph_objs as go\n",
    "import random\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "r = lambda: random.randint(0,255)\n",
    "plot_data = df.copy()\n",
    "if 't-SNE-X' not in plot_data.columns:\n",
    "    print('TSNE coordinates have not been added. Please run the desired clusterizer again in the corresponding notebook')\n",
    "else:\n",
    "    traces = []\n",
    "    for c in plot_data.clusters.unique():\n",
    "        trace = go.Scatter(\n",
    "            x=plot_data[plot_data[\"clusters\"]==c][\"t-SNE-X\"],\n",
    "            y=plot_data[plot_data[\"clusters\"]==c][\"t-SNE-Y\"],\n",
    "            mode = 'markers',\n",
    "            name = str(c),\n",
    "            marker = go.scatter.Marker(size=4, color='#%02X%02X%02X' % (r(),r(),r())),\n",
    "            showlegend = True,\n",
    "        )\n",
    "        traces.append(trace)\n",
    "\n",
    "    data = traces\n",
    "\n",
    "    # Plot and embed in ipython notebook\n",
    "    #fname = '../' + d.value + '/figures/' + vec + clu[:-4] \n",
    "\n",
    "    plotly.offline.iplot(data,image_width=1280, image_height=800, image='png', filename='plot_image')\n",
    "\n",
    "    #plotly.offline.plot(data, filename=fname+'.html', auto_open=False,\n",
    "                        #image_width=1280, image_height=800)\n",
    "                        #,image='png', image_filename='plot_image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Bulk.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['birth_place'].notnull(),\"clusters\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[(df['birth_place'] == \"Y\") & (df['Bulk'] == \"N\"),\"clusters\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[(df['birth_place'] == \"N\") & (df['Bulk'] == \"N\"),\"clusters\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
