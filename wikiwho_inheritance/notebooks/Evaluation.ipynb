{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics.cluster import v_measure_score\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from IPython.display import clear_output\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('/home/stankeaa/wikiwho_inheritance/')\n",
    "import re\n",
    "from sequencers.clusterers import kmeans, dbscan\n",
    "%matplotlib inline  \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getting_vectorizers(dirs):\n",
    "    dir_to_vecs = {}\n",
    "    for folder in dirs:\n",
    "        dir_to_vecs[folder] = os.listdir(sys.path[-1] + folder + '/clusterers/')[1:]\n",
    "    return dir_to_vecs\n",
    "\n",
    "def getting_pickles(vecs):\n",
    "    vecs_to_clusters = {}\n",
    "    for vectorizer in vecs:\n",
    "        path = sys.path[-1] + folder + '/clusterers/' + vectorizer + '/tsne/'\n",
    "        pickles = []\n",
    "        for dirpath, dirnames, files in os.walk(path):\n",
    "            for file_name in files:\n",
    "                pickles.append(file_name)\n",
    "        vecs_to_clusters[vectorizer] = pickles        \n",
    "    return vecs_to_clusters\n",
    "\n",
    "dirs = []\n",
    "for file in os.listdir( sys.path[-1] ):\n",
    "    if re.match(\"^[0-9_-]*$\", file):\n",
    "        dirs.append(file) # getting the chobs\n",
    "dir_to_vecs = getting_vectorizers(dirs) # getting the vectorizers\n",
    "for folder in dir_to_vecs:\n",
    "    vecs = dir_to_vecs[folder]\n",
    "    vecs_to_clusters = getting_pickles(vecs) # getting the clusterers (with tsne)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V-scores by vectorizer, comparing the clusterizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 15, 5\n",
    "for vec in vecs_to_clusters:\n",
    "    v_measure_scores = {}\n",
    "    for clu in vecs_to_clusters[vec]:\n",
    "        df = pd.read_pickle('../39570/clusterers/' + vec + '/tsne/' + clu)\n",
    "        nationality = df.loc[~df['nationality'].isna()]\n",
    "        score = v_measure_score(nationality['nationality'], nationality['clusters'])\n",
    "        if clu.split('_')[0] == 'Token':\n",
    "            if 'id' in clu:\n",
    "                clu = 'Token_Sim_ID'\n",
    "            else:\n",
    "                clu = 'Token_Sim_Str'\n",
    "        else:\n",
    "            clu = clu.split('_')[0]\n",
    "        if clu in v_measure_scores.keys() and score > v_measure_scores[clu]: \n",
    "                v_measure_scores[clu] = score\n",
    "        elif clu not in v_measure_scores.keys():\n",
    "            v_measure_scores[clu] = score\n",
    "        \n",
    "    plt.bar(range(len(v_measure_scores)), list(v_measure_scores.values()), align='center')       \n",
    "    plt.xticks(range(len(v_measure_scores)), list(v_measure_scores.keys()))\n",
    "    print(vec)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V-scores by clasterizer, comparing the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_data(vectorizer):\n",
    "    if vectorizer is not None:\n",
    "        print('Downloaded data for {} vectorizer'.format(vectorizer)) \n",
    "    global vec, df\n",
    "    vec = vectorizer\n",
    "    #df = pd.read_pickle(sys.path[-1] + d.value + '/clusterers/' + vec + '/tsne/' + clu)\n",
    "    \n",
    "def select_vec(folder):\n",
    "    v.options = dir_to_vecs[folder]\n",
    "\n",
    "#add in 'select clusterizer' function that looks in the new dictionary\n",
    "def select_clusterizer(vectorizer):\n",
    "    c.options = vecs_to_clusters[vectorizer]\n",
    "\n",
    "d = widgets.Dropdown(options=dir_to_vecs.keys())\n",
    "init = d.value\n",
    "v = widgets.Dropdown(options=dir_to_vecs[init])\n",
    "\n",
    "\n",
    "init2= v.value #new start value for vectorizer dropdown\n",
    "\n",
    "j = widgets.interactive(print_data, vectorizer=v) #define clusterizer value\n",
    "i = widgets.interactive(select_vec, folder=d)\n",
    "\n",
    "display(i)\n",
    "display(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_measure_scores_db = {}\n",
    "v_measure_scores_ts = {}\n",
    "#global_min = 100\n",
    "for clu in vecs_to_clusters[vec]:\n",
    "    if clu.split('_')[0] == 'DBscan':\n",
    "        eps = clu.split('_')[5][:-4]\n",
    "        min_samples = clu.split('_')[3]\n",
    "        if min_samples not in v_measure_scores_db.keys():\n",
    "            v_measure_scores_db[min_samples] = []\n",
    "        df = pd.read_pickle('../39570/clusterers/' + vec + '/tsne/' + clu)\n",
    "        nationality = df.loc[~df['nationality'].isna()]\n",
    "        score = v_measure_score(nationality['nationality'], nationality['clusters'])\n",
    "        #if int(min_samples) < global_min:\n",
    "            #global_min = int(min_samples)\n",
    "        v_measure_scores_db[min_samples].append((score, float(eps)))\n",
    "    elif clu.split('_')[0] == 'Token':\n",
    "        if 'id' in clu:\n",
    "            intersect = clu.split('_')[3]\n",
    "            df = pd.read_pickle('../39570/clusterers/' + vec + '/tsne/' + clu)\n",
    "            nationality = df.loc[~df['nationality'].isna()]\n",
    "            score = v_measure_score(nationality['nationality'], nationality['clusters'])\n",
    "            v_measure_scores_ts[int(intersect)] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Running k-means with different k to select an optimal, takes time'''\n",
    "def transform_feat(df):\n",
    "    features = pd.DataFrame()\n",
    "    for i, row in df.iterrows():\n",
    "        feat = pd.Series(row['features'])\n",
    "        features = features.append(feat,ignore_index=True)\n",
    "    return features\n",
    "\n",
    "v_measure_scores_km = {}\n",
    "K = range(4,15)\n",
    "for k in K:\n",
    "    km = KMeans(n_clusters=k)\n",
    "    km.fit(transform_feat(df))\n",
    "    df['clusters'] = km.labels_\n",
    "    nationality = df.loc[~df['nationality'].isna()]\n",
    "    score = v_measure_score(nationality['nationality'], nationality['clusters'])\n",
    "    v_measure_scores_km[k] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Clusterizers for ' + vec)\n",
    "if len(v_measure_scores_db) > 0:\n",
    "    plt.figure()\n",
    "    for min_sample in v_measure_scores_db.keys():\n",
    "        v_measure_scores_sort = sorted(v_measure_scores_db[min_sample], key=lambda tup: tup[1])\n",
    "        x_val = [x[1] for x in v_measure_scores_sort]\n",
    "        y_val = [x[0] for x in v_measure_scores_sort]\n",
    "        plt.plot(x_val, y_val, label='Min sample ' + min_sample)\n",
    "\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title(\"DBscan\")\n",
    "    plt.xlabel('Eps')\n",
    "    plt.ylabel('V-measure score')\n",
    "    plt.show()\n",
    "\n",
    "if len(v_measure_scores_ts) > 0:\n",
    "    plt.figure()\n",
    "    ts = sorted(v_measure_scores_ts.items()) \n",
    "    x, y = zip(*ts)\n",
    "    plt.plot(x, y)\n",
    "    plt.title(\"Token Similarity ID\")\n",
    "    plt.xlabel('Intersection')\n",
    "    plt.ylabel('V-measure score')\n",
    "    #plt.figure(figsize=(8, 6))\n",
    "    plt.show()\n",
    "    \n",
    "if len(v_measure_scores_km) > 0:\n",
    "    plt.figure()\n",
    "    km = sorted(v_measure_scores_km.items()) \n",
    "    x, y = zip(*km)\n",
    "    plt.plot(x, y)\n",
    "    plt.title(\"Kmeans\")\n",
    "    plt.xlabel('number of clusters')\n",
    "    plt.ylabel('V-measure score')\n",
    "    #plt.figure(figsize=(8, 6))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score for one pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_data(vectorizer,clusterizer):\n",
    "    if clusterizer is not None:\n",
    "        print('Downloaded data for {} vectorizer and {} clusterizer'.format(vectorizer, clusterizer[:-4])) \n",
    "    global vec, clu, df\n",
    "    vec, clu = vectorizer, clusterizer\n",
    "    df = pd.read_pickle(sys.path[-1] + d.value + '/clusterers/' + vec + '/tsne/' + clu)\n",
    "    \n",
    "def select_vec(folder):\n",
    "    v.options = dir_to_vecs[folder]\n",
    "\n",
    "#add in 'select clusterizer' function that looks in the new dictionary\n",
    "def select_clusterizer(vectorizer):\n",
    "    c.options = vecs_to_clusters[vectorizer]\n",
    "\n",
    "d = widgets.Dropdown(options=dir_to_vecs.keys())\n",
    "init = d.value\n",
    "v = widgets.Dropdown(options=dir_to_vecs[init])\n",
    "\n",
    "\n",
    "init2= v.value #new start value for vectorizer dropdown\n",
    "c = widgets.Dropdown(options=vecs_to_clusters[init2]) #define district dropdown widget\n",
    "\n",
    "j = widgets.interactive(print_data, vectorizer=v, clusterizer=c) #define clusterizer value\n",
    "i = widgets.interactive(select_vec, folder=d)\n",
    "k = widgets.interactive(select_clusterizer, vectorizer=v) #call everything together with new interactive\n",
    "\n",
    "display(i)\n",
    "display(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nationality = df.loc[~df['nationality'].isna()]\n",
    "v_score = v_measure_score(nationality['nationality'], nationality['clusters'])\n",
    "n_score = normalized_mutual_info_score(nationality['nationality'], nationality['clusters'])\n",
    "print('For vectorizer ', vec, 'and clusterizer ', clu, '\\nthe v-measure score is: ', v_score, '\\nthe normalized mutual info score is: ', n_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy, general for one pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import entropy\n",
    "nationality = df.loc[~df['nationality'].isna()]\n",
    "\n",
    "def get_weighted_entropy(dataframe, entropy_column, group_columns=\"cluster\"):\n",
    "    group_size = dataframe.groupby(group_columns).size()\n",
    "    group_entropy = dataframe.groupby(group_columns)[entropy_column].apply(lambda x: entropy(x.value_counts().values))\n",
    "    absolute_entropy = group_size * group_entropy\n",
    "    weighted_entropy = (group_size * group_entropy).mean()\n",
    "    return group_entropy, weighted_entropy, group_size\n",
    "\n",
    "group_entropy, weighted_entropy, group_size = get_weighted_entropy(nationality, group_columns=\"clusters\", entropy_column=\"nationality\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('General entropy for ' + vec + ' ' + clu[:-4])\n",
    "entropy_df = pd.DataFrame(group_entropy).rename(columns={\"nationality\": \"entropy\"})\n",
    "entropy_df = entropy_df.merge(pd.DataFrame(data = group_size), left_index=True, right_index=True).rename(columns={0: \"size\"})\n",
    "entropy_df = entropy_df.sort_values(by='entropy', ascending=False)\n",
    "entropy_df.index  = entropy_df.index.map(str)\n",
    "entropy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 0.7\n",
    "entropy_df['entropy'].plot(kind='bar', width = width)\n",
    "plt.ylabel('Entropy')\n",
    "plt.title('Entropy chart for ' + vec + ' ' + clu[:-4])\n",
    "entropy_df['size'].plot(secondary_y=True, color='r')\n",
    "plt.ylabel('Cluster size')\n",
    "plt.legend()\n",
    "plt.xlim([-width, len(entropy_df['entropy'])-width])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General entropy, for one clusterizers. Comparing the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_db = {}\n",
    "entropy_ts = {}\n",
    "for clu in vecs_to_clusters[vec]:\n",
    "    if clu.split('_')[0] == 'DBscan':\n",
    "        eps = clu.split('_')[5][:-4]\n",
    "        min_samples = clu.split('_')[3]\n",
    "        if min_samples not in entropy_db.keys():\n",
    "            entropy_db[min_samples] = []\n",
    "        df = pd.read_pickle('../39570/clusterers/' + vec + '/tsne/' + clu)\n",
    "        nationality = df.loc[~df['nationality'].isna()]\n",
    "        entropy_val, weighted_entropy, group_size = get_weighted_entropy(nationality, group_columns=\"clusters\", entropy_column=\"nationality\")\n",
    "        entropy_db[min_samples].append((weighted_entropy, float(eps)))\n",
    "    elif clu.split('_')[0] == 'Token':\n",
    "        if 'id' in clu:\n",
    "            intersect = clu.split('_')[3]\n",
    "            df = pd.read_pickle('../39570/clusterers/' + vec + '/tsne/' + clu)\n",
    "            nationality = df.loc[~df['nationality'].isna()]\n",
    "            entropy_val, weighted_entropy, group_size = get_weighted_entropy(nationality, group_columns=\"clusters\", entropy_column=\"nationality\")\n",
    "            entropy_ts[int(intersect)] = weighted_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('General entropy for clusterizers for ' + vec)\n",
    "if len(entropy_db) > 0:\n",
    "    plt.figure()\n",
    "    for min_sample in entropy_db.keys():\n",
    "        entropy_sort = sorted(entropy_db[min_sample], key=lambda tup: tup[1])\n",
    "        x_val = [x[1] for x in entropy_sort]\n",
    "        y_val = [x[0] for x in entropy_sort]\n",
    "        plt.plot(x_val, y_val, label='Min sample ' + min_sample)\n",
    "\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.title(\"DBscan\")\n",
    "    plt.xlabel('Eps')\n",
    "    plt.ylabel('Entropy')\n",
    "    plt.show()\n",
    "\n",
    "if len(entropy_ts) > 0:\n",
    "    plt.figure()\n",
    "    ts = sorted(entropy_ts.items()) \n",
    "    x, y = zip(*ts)\n",
    "    plt.plot(x, y)\n",
    "    plt.title(\"Token Similarity ID\")\n",
    "    plt.xlabel('Intersection')\n",
    "    plt.ylabel('Entropy')\n",
    "    #plt.figure(figsize=(8, 6))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Position entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nationality = df.loc[~df['nationality'].isna()] #taking only nationality cases\n",
    "cluster_sizes = nationality.groupby(\"clusters\").size().values\n",
    "cluster_sizes = cluster_sizes / cluster_sizes.sum()\n",
    "\n",
    "nationality['relative_position'] = nationality.loc[: , ['left_neigh', 'right_neigh']].mean(axis=1)\n",
    "position_entropy_clusters = nationality.groupby(\"clusters\")[\"relative_position\"].apply(\n",
    "    lambda x: entropy(x.value_counts().values))\n",
    "position_entropy_clusters.plot(kind='bar')\n",
    "plt.ylabel('Position entropy')\n",
    "plt.title('Position entropy chart for ' + vec + ' ' + clu[:-4] + ' based on mean position')\n",
    "position_entropy = (cluster_sizes * position_entropy_clusters).sum()\n",
    "avg_position_entropy =  position_entropy_clusters.mean()\n",
    "position_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "token_entropy_clusters = nationality.groupby(\"clusters\")[\"ins_tokens_str\"].apply(\n",
    "    lambda token_tuples: entropy(pd.Series(\n",
    "        [token for token_tuple in token_tuples.tolist() for token in token_tuple]\n",
    "    ).value_counts().values))\n",
    "\n",
    "token_entropy_clusters.plot(kind='bar')\n",
    "plt.ylabel('Token entropy')\n",
    "plt.title('Token entropy chart for ' + vec + ' ' + clu[:-4] + ' based on inserted string tokens')\n",
    "position_entropy = (cluster_sizes * position_entropy_clusters).sum()\n",
    "token_entropy = (cluster_sizes * token_entropy_clusters).sum()\n",
    "avg_token_entropy =  token_entropy_clusters.mean()\n",
    "token_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing position and token entropies for different parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_pos = {}\n",
    "entropy_tok = {}\n",
    "for clu in vecs_to_clusters[vec]:\n",
    "    if clu.split('_')[0] == 'DBscan':\n",
    "        eps = clu.split('_')[5][:-4]\n",
    "        min_samples = clu.split('_')[3]\n",
    "        if min_samples not in entropy_pos.keys(): #adding new min_sample as a key to the dict\n",
    "            entropy_pos[min_samples] = []\n",
    "        if min_samples not in entropy_tok.keys(): #adding new min_sample as a key to the dict\n",
    "            entropy_tok[min_samples] = []\n",
    "        df = pd.read_pickle('../39570/clusterers/' + vec + '/tsne/' + clu)\n",
    "        #nationality = df.loc[~df['nationality'].isna()] #taking only nationality cases\n",
    "        df['relative_position'] = df.loc[: , ['left_neigh', 'right_neigh']].mean(axis=1) # calculating relative position as a mean between the beggining and the end of the context\n",
    "        cluster_sizes = df.groupby(\"clusters\").size().values\n",
    "        cluster_sizes = cluster_sizes / cluster_sizes.sum()\n",
    "        position_entropy_clusters = df.groupby(\"clusters\")[\"relative_position\"].apply(\n",
    "            lambda x: entropy(x.value_counts().values)) #calculating the entropy itself\n",
    "        position_entropy = (cluster_sizes * position_entropy_clusters).sum() #calculating the total weighted position entropy\n",
    "        entropy_pos[min_samples].append((position_entropy, float(eps))) #appending a value to the list for respective min_sample\n",
    "        \n",
    "        token_entropy_clusters = df.groupby(\"clusters\")[\"ins_tokens_str\"].apply(\n",
    "        lambda token_tuples: entropy(pd.Series(\n",
    "            [token for token_tuple in token_tuples.tolist() for token in token_tuple]\n",
    "        ).value_counts().values))\n",
    "        token_entropy = (cluster_sizes * token_entropy_clusters).sum() # calculating the total weighted token entropy\n",
    "        entropy_tok[min_samples].append((token_entropy, float(eps))) #appending a value to the list for respective min_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Position and token entropy for clusterizers for ' + vec)\n",
    "if len(entropy_pos) > 0:\n",
    "    plt.figure()\n",
    "    for min_sample in entropy_pos.keys():\n",
    "        entropy_sort = sorted(entropy_pos[min_sample], key=lambda tup: tup[1])\n",
    "        x_val = [x[1] for x in entropy_sort]\n",
    "        y_val = [x[0] for x in entropy_sort]\n",
    "        plt.plot(x_val, y_val, label='Min sample ' + min_sample)\n",
    "\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.title(\"Position entropy\")\n",
    "    plt.xlabel('Eps')\n",
    "    plt.ylabel('Entropy')\n",
    "    plt.show()\n",
    "\n",
    "if len(entropy_tok) > 0:\n",
    "    plt.figure()\n",
    "    for min_sample in entropy_tok.keys():\n",
    "        entropy_sort = sorted(entropy_tok[min_sample], key=lambda tup: tup[1])\n",
    "        x_val = [x[1] for x in entropy_sort]\n",
    "        y_val = [x[0] for x in entropy_sort]\n",
    "        plt.plot(x_val, y_val, label='Min sample ' + min_sample)\n",
    "\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.title(\"Token entropy\")\n",
    "    plt.xlabel('Eps')\n",
    "    plt.ylabel('Entropy')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Silhouette score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the average score and silhouette cluster scores from the df\n",
    "sil_avg = df.loc[0, 'avg_sil']\n",
    "sil_df = df.copy()\n",
    "if clu.split('_')[0] == 'Kmeans':\n",
    "    cl = sil_df.clusters.unique()\n",
    "elif clu.split('_')[0] == 'DBscan':    \n",
    "    cl = sil_df.clusters.value_counts()[1:11].index\n",
    "    \n",
    "    \n",
    "plt.figure(figsize=(18, 7))\n",
    "plt.xlim(-1, 1)\n",
    "print(\"For n_clusters =\", len(cl),\n",
    "      \"The average silhouette_score is :\", sil_avg)\n",
    "y_lower = 10\n",
    "for i in cl:\n",
    "    # Aggregate the silhouette scores for samples belonging to\n",
    "    # cluster i, and sort them\n",
    "    ith_cluster_silhouette_values = \\\n",
    "        sil_df[sil_df[\"clusters\"]==i][\"silhouette_value\"]\n",
    "\n",
    "    ith_cluster_silhouette_values.sort_values()\n",
    "\n",
    "    size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "    y_upper = y_lower + size_cluster_i\n",
    "\n",
    "    plt.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                      0, ith_cluster_silhouette_values,\n",
    "                       alpha=0.7)\n",
    "\n",
    "    # Label the silhouette plots with their cluster numbers at the middle\n",
    "    plt.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "    # Compute the new y_lower for next plot\n",
    "    y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "plt.title(\"The silhouette plot for the various clusters.\")\n",
    "plt.xlabel(\"The silhouette coefficient values\")\n",
    "plt.ylabel(\"Cluster label\")\n",
    "\n",
    "# The vertical line for average silhouette score of all the values\n",
    "plt.axvline(x=sil_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "plt.yticks([])  # Clear the yaxis labels / ticks\n",
    "plt.xticks([-1, -0.6,-0.4,-0.2,-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "plt.show()\n",
    "#plt.savefig(fname+'_silhouette.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "def transform_feat(df):\n",
    "        features = pd.DataFrame()\n",
    "        for i, row in df.iterrows():\n",
    "            feat = pd.Series(row['features'])\n",
    "            features = features.append(feat,ignore_index=True)\n",
    "        return features\n",
    "\n",
    "ns = 5\n",
    "nbrs = NearestNeighbors(n_neighbors=ns).fit(transform_feat(df))\n",
    "distances, indices = nbrs.kneighbors(transform_feat(df))\n",
    "distanceDec = sorted(distances[:,ns-1])#, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(list(range(1,len(df)+1)), distanceDec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
