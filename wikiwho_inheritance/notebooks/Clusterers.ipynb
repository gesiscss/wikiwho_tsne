{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle \n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('/home/stankeaa/wikiwho_inheritance/')\n",
    "from sequencers.clusterers import kmeans, dbscan, token_similarity\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import re\n",
    "from sequencers import tsne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs = []\n",
    "for file in os.listdir( sys.path[-1] ):\n",
    "    if re.match(\"^[0-9_-]*$\", file):\n",
    "        dirs.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs = []\n",
    "for item in os.listdir(sys.path[-1] + a.value + '/vectorizers/'):\n",
    "    if item != '.ipynb_checkpoints':\n",
    "        vecs.append(item.split('___')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clusterer\n",
    "This notebook clusterizes wikiwho chobs. \n",
    "It requires an **existing pickle** with chobs already vectorized (e.g. with **features** added). <br>\n",
    "The following clustering methods are possible: \n",
    "- Kmeans (parameter: *random state*)\n",
    "- DBscan (parameters: *min number of samples in one cluster* and *eps*)\n",
    "- Token similarity (parameter: *intersection*, or min number of tokens to be the same in context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = widgets.Dropdown(\n",
    "    options = dirs,\n",
    "    value = None,\n",
    "    description = 'Article id:',\n",
    ")\n",
    "v = widgets.Dropdown(\n",
    "    options = [],\n",
    "    value = None,\n",
    "    description = 'Vectorizer:',\n",
    ")\n",
    "c = widgets.Dropdown(\n",
    "    options = ['kmeans.Kmeans', 'dbscan.DBscan', 'token_similarity.Token_Similarity'],\n",
    "    description = 'Clusterer:',\n",
    ")\n",
    "p1 = widgets.IntText(\n",
    "    value=None\n",
    ")\n",
    "p2 = widgets.FloatText(\n",
    "    value=None\n",
    ")\n",
    "p3 = widgets.Dropdown(\n",
    "    options = ['id', 'string'],\n",
    "    description = 'Token type:',\n",
    ")\n",
    "ts = widgets.IntText(\n",
    "    value=42,\n",
    "    description = 'TSNE random state:',\n",
    ")\n",
    "\n",
    "def a_on_change(change):\n",
    "    if change['type'] == 'change' and change['name'] == 'value':\n",
    "        v.options = os.listdir(sys.path[-1] + change['new'] + '/vectorizers/')\n",
    "        display(v, c, ts)\n",
    "        \n",
    "def v_on_change(change):\n",
    "    if change['type'] == 'change' and change['name'] == 'value':\n",
    "        global vectorizer\n",
    "        vectorizer = change['new']\n",
    "\n",
    "def c_on_change(change):\n",
    "    if change['type'] == 'change' and change['name'] == 'value':\n",
    "        global clusterer\n",
    "        clusterer = eval(c.value)\n",
    "        clear_output()\n",
    "        display(a, v, c, ts)\n",
    "        if change['new'] == 'kmeans.Kmeans':\n",
    "            p1.value = 42\n",
    "            p1.description = 'Random state: '\n",
    "            display(p1)\n",
    "        elif change['new'] == 'token_similarity.Token_Similarity':\n",
    "            p1.value = 2\n",
    "            p1.description = 'Intersection: '\n",
    "            display(p1, p3)\n",
    "        else:\n",
    "            p1.value = 4\n",
    "            p1.description = 'Min samples: '\n",
    "            p2.value = 1.5\n",
    "            p2.description = 'Eps: '\n",
    "            display(p1, p2)\n",
    "        print('Data is saved')\n",
    "\n",
    "a.observe(a_on_change)\n",
    "v.observe(v_on_change)\n",
    "c.observe(c_on_change)\n",
    "display(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = '../'+a.value +'/clusterers/'+vectorizer[:-4]\n",
    "if not os.path.exists(save_path):\n",
    "    os.mkdir(save_path+'/') \n",
    "if clusterer == kmeans.Kmeans:\n",
    "    cl = clusterer(pd.read_pickle('../'+a.value +'/vectorizers/'+vectorizer), {'random_state':p1.value}, save_path)\n",
    "elif clusterer == token_similarity.Token_Similarity:\n",
    "    cl = clusterer(pd.read_pickle('../'+a.value +'/vectorizers/'+vectorizer), {'intersection':p1.value, 'token_type':p3.value}, save_path)\n",
    "else:\n",
    "    cl = clusterer(pd.read_pickle('../'+a.value +'/vectorizers/'+vectorizer), {'min_samples':p1.value, 'eps':p2.value}, save_path)\n",
    "cl.get_clusters()\n",
    "cl.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(save_path+'/tsne/'):\n",
    "    os.mkdir(save_path+'/tsne/') \n",
    "ft = tsne.Tsne(pd.read_pickle(cl.dirpath + '.pkl'), {'random_state':ts.value}, save_path+'/tsne/'+cl.dirpath.split('/')[-1])\n",
    "ft.dirpath = ft.path\n",
    "ft.get_plot_data(cl.df)\n",
    "ft.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strin_clean = pd.read_pickle('../39570/clusterers/Word_Embed_word_embed_size_300_use_gap_True_context_5___Chobs_context_5_gap_length_20/tsne/Token_Similarity_intersection_2_token_type_string.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strin_dirty = pd.read_pickle('../39570/clusterers/Word_Embed_word_embed_size_300_use_gap_True_left_context_5_right_context_5/tsne/Token_Similarity_intersection_2_token_type_string.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids['clusters']\n",
    "#ids.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strin['clusters']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strin_dirty['clusters']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
