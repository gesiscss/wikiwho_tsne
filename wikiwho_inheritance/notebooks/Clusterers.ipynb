{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle \n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('/home/stankeaa/wikiwho_inheritance/')\n",
    "from sequencers.clusterers import kmeans, dbscan, token_similarity, lda\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import re\n",
    "from sequencers import tsne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs = []\n",
    "for file in os.listdir( sys.path[-1] ):\n",
    "    if re.match(\"^[0-9_-]*$\", file):\n",
    "        dirs.append(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clusterer\n",
    "This notebook clusterizes wikiwho chobs. \n",
    "It requires an **existing pickle** with chobs already vectorized (e.g. with **features** added). <br>\n",
    "The following clustering methods are possible: \n",
    "- Kmeans (parameter: *random state*)\n",
    "- DBscan (parameters: *min number of samples in one cluster* and *eps*)\n",
    "- Token similarity (parameter: *intersection*, or min number of tokens to be the same in context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = widgets.Dropdown(\n",
    "    options = dirs,\n",
    "    value = None,\n",
    "    description = 'Article id:',\n",
    ")\n",
    "v = widgets.Dropdown(\n",
    "    options = [],\n",
    "    value = None,\n",
    "    description = 'Vectorizer:',\n",
    ")\n",
    "c = widgets.Dropdown(\n",
    "    options = ['kmeans.Kmeans', 'dbscan.DBscan', 'token_similarity.Token_Similarity', 'lda.LDA'],\n",
    "    description = 'Clusterer:',\n",
    ")\n",
    "min_s = widgets.IntText(\n",
    "    value=5,\n",
    "    description = 'Min samples:'\n",
    ")\n",
    "p1 = widgets.IntText(\n",
    "    value=None\n",
    ")\n",
    "p2 = widgets.FloatText(\n",
    "    value=None\n",
    ")\n",
    "p3 = widgets.Dropdown(\n",
    "    options = ['id', 'string'],\n",
    "    description = 'Token type:',\n",
    ")\n",
    "ts = widgets.IntText(\n",
    "    value=42,\n",
    "    description = 'TSNE random state:'\n",
    ")\n",
    "\n",
    "def a_on_change(change):\n",
    "    if change['type'] == 'change' and change['name'] == 'value':\n",
    "        v.options = os.listdir(sys.path[-1] + change['new'] + '/vectorizers/')\n",
    "        display(v, c, min_s, ts)\n",
    "        \n",
    "def v_on_change(change):\n",
    "    if change['type'] == 'change' and change['name'] == 'value':\n",
    "        global vectorizer\n",
    "        vectorizer = change['new']\n",
    "\n",
    "def c_on_change(change):\n",
    "    if change['type'] == 'change' and change['name'] == 'value':\n",
    "        global clusterer\n",
    "        clusterer = eval(c.value)\n",
    "        clear_output()\n",
    "        display(a, v, c, min_s, ts)\n",
    "        if change['new'] == 'kmeans.Kmeans':\n",
    "            p1.value = 42\n",
    "            p1.description = 'Random state: '\n",
    "            display(p1)\n",
    "        elif change['new'] == 'token_similarity.Token_Similarity':\n",
    "            p1.value = 2\n",
    "            p1.description = 'Intersection: '\n",
    "            display(p1, p3)\n",
    "        elif change['new'] == 'dbscan.DBscan':\n",
    "            p2.value = 1.5\n",
    "            p2.description = 'Eps: '\n",
    "            display(p2)\n",
    "        print('Data is saved')\n",
    "\n",
    "a.observe(a_on_change)\n",
    "v.observe(v_on_change)\n",
    "c.observe(c_on_change)\n",
    "display(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FOR CHOOSING NUM_TOPICS FOR LDA\n",
    "save_path = '../'+a.value +'/clusterers/'+vectorizer[:-4]\n",
    "cl = lda.LDA(pd.read_pickle('../'+a.value +'/vectorizers/'+vectorizer), {'num_topics':0}, save_path)\n",
    "cl.get_corpus()\n",
    "model_list, coherence_values = cl.choose_lda_model(start=20, limit=100, step=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = '../'+a.value +'/clusterers/'+vectorizer[:-4]\n",
    "if not os.path.exists(save_path):\n",
    "    os.mkdir(save_path+'/') \n",
    "if clusterer == kmeans.Kmeans:\n",
    "    cl = clusterer(pd.read_pickle('../'+a.value +'/vectorizers/'+vectorizer), {'random_state':p1.value, 'min_samples':min_s.value}, save_path)\n",
    "    cl.df['avg_sil'] = np.nan\n",
    "    cl.df.loc[0, 'avg_sil'] = cl.silhouette()\n",
    "elif clusterer == token_similarity.Token_Similarity:\n",
    "    cl = clusterer(pd.read_pickle('../'+a.value +'/vectorizers/'+vectorizer), {'intersection':p1.value, 'token_type':p3.value, 'min_samples':min_s.value}, save_path)\n",
    "elif clusterer == lda.LDA:\n",
    "    cl = clusterer(pd.read_pickle('../'+a.value +'/vectorizers/'+vectorizer), {'num_topics':37, 'min_samples':min_s.value}, save_path)\n",
    "    cl.get_corpus()\n",
    "\n",
    "else:\n",
    "    cl = clusterer(pd.read_pickle('../'+a.value +'/vectorizers/'+vectorizer), {'min_samples':min_s.value, 'eps':p2.value}, save_path)\n",
    "    cl.df['avg_sil'] = np.nan\n",
    "    cl.df.loc[0, 'avg_sil'] = cl.silhouette()\n",
    "cl.get_clusters()\n",
    "cl.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(save_path+'/tsne/'):\n",
    "    os.mkdir(save_path+'/tsne/') \n",
    "ft = tsne.Tsne(pd.read_pickle(cl.dirpath + '.pkl'), {'random_state':ts.value}, save_path+'/tsne/'+cl.dirpath.split('/')[-1])\n",
    "ft.dirpath = ft.path\n",
    "ft.get_plot_data(cl.df)\n",
    "ft.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('../39570/clusterers/Distilbert_LR___Chobs_context_5_gap_length_20/tsne/DBscan_min_samples_5_eps_1.0.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_other = pd.read_pickle('../39570/clusterers/Distilbert_LR___Chobs_context_5_gap_length_20/tsne/DBscan_min_samples_6_eps_1.0.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = '../'+a.value +'/clusterers/'+vectorizer[:-4]\n",
    "cl = clusterer(pd.read_pickle('../'+a.value +'/vectorizers/'+vectorizer), {'random_state':p1.value, 'min_samples':min_s.value}, save_path)\n",
    "Sum_of_squared_distances = cl.evaluate_k()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = range(1,15)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(K, Sum_of_squared_distances, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Sum_of_squared_distances')\n",
    "plt.title('Elbow Method For Optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
