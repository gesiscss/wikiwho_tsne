{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import gensim\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# load id->word mapping (the dictionary), one of the results of step 2 above\n",
    "id2word = gensim.corpora.Dictionary.load_from_text('~/wiki_corpus/wiki_corpus_wordids.txt')\n",
    "# load corpus iterator\n",
    "mm = gensim.corpora.MmCorpus('~/wiki_corpus/wiki_corpus_bow.mm')\n",
    "# mm = gensim.corpora.MmCorpus('wiki_en_tfidf.mm.bz2') # use this if you compressed the TFIDF output (recommended)\n",
    "\n",
    "print(mm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_model = gensim.models.TfidfModel.load('~/wiki_corpus/wiki_corpus.tfidf_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle  \n",
    "\n",
    "with open(\"wiki_corpus_bow.mm.metadata.cpickle\", 'rb') as meta_file:\n",
    "    docno2metadata = pickle.load(meta_file)\n",
    "\n",
    "_dict = {v: i for i, (k, v)  in docno2metadata.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_id = 'John Logie Baird'\n",
    "doc_num = _dict[page_id]\n",
    "print(\"Title: {}\".format(docno2metadata[doc_num][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = tfidf_model[mm[doc_num]]\n",
    "vector = sorted(vec, key=lambda tup: tup[1], reverse=True)\n",
    "for pair in vector:\n",
    "    print(id2word.get(pair[0]), ',', 'tfidf:', pair[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wikiwho_wrapper import WikiWho\n",
    "ww = WikiWho(lng='en')\n",
    "df = ww.dv.last_rev_content(article=page_id) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ww_text = ''\n",
    "for word in df['token']:\n",
    "    ww_text = ww_text + ' ' + word\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ww_text = ww_text.replace('[[', '').replace(']]', '').replace('}}', '').replace('{{', '').replace('–', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rake_nltk import Rake\n",
    "\n",
    "r = Rake() # Uses stopwords for english from NLTK, and all puntuation characters.\n",
    "r.extract_keywords_from_text(ww_text)\n",
    "wd = r.get_word_degrees()\n",
    "sorted(wd.items(), key=lambda kv: kv[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF and RAKE with chobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wikiwho_chobj import Chobjer\n",
    "import pandas as pd\n",
    "co = Chobjer(article=\"39570\", pickles_path='../../bert', lang='en', context=5)\n",
    "chobs = pd.DataFrame(co.iter_chobjs(), columns = next(co.iter_chobjs()).keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "right_tokens = []\n",
    "left_tokens = []\n",
    "for i, row in chobs.iterrows():\n",
    "    left_tokens.append(' '.join(word for word in chobs['left_token_str'][i]))\n",
    "    right_tokens.append(' '.join(word for word in chobs['right_token_str'][i]))\n",
    "    left_tokens[i] = left_tokens[i].replace('[[', '').replace(']]', '').replace('}}', '').replace('{{', '').replace('–', '').replace('\\'', '')\n",
    "    right_tokens[i] = right_tokens[i].replace('[[', '').replace(']]', '').replace('}}', '').replace('{{', '').replace('–', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_words = [\n",
    "    [word for word in line.lower().split() if word not in stopwords.words('english')]\n",
    "    for line in left_tokens\n",
    "]\n",
    "\n",
    "dct_left = gensim.corpora.Dictionary(left_words)\n",
    "corpus_left = [dct_left.doc2bow(word) for word in left_words]\n",
    "model_left = gensim.models.TfidfModel(corpus_left)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, item in enumerate(corpus_left):\n",
    "    vec = model_left[corpus_left[i]]\n",
    "    vector = sorted(vec, key=lambda tup: tup[1], reverse=True)\n",
    "    for pair in vector:\n",
    "        print(dct_left.get(pair[0]), ',', 'tfidf:', pair[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "right_words = [\n",
    "    [word for word in line.lower().split() if word not in stopwords.words('english')]\n",
    "    for line in right_tokens\n",
    "]\n",
    "\n",
    "dct_right = gensim.corpora.Dictionary(right_words)\n",
    "corpus_right = [dct_right.doc2bow(word) for word in right_words]\n",
    "model_right = gensim.models.TfidfModel(corpus_right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, item in enumerate(corpus_right):\n",
    "    vec = model_right[corpus_right[i]]\n",
    "    vector = sorted(vec, key=lambda tup: tup[1], reverse=True)\n",
    "    for pair in vector:\n",
    "        print(dct_left.get(pair[0]), ',', 'tfidf:', pair[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
