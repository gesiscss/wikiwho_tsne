{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from wikiwho_chobj import Chobjer\n",
    "\n",
    "context = 5\n",
    "co = Chobjer(article=\"39570\", pickles_path='../../bert', lang='en', context=context)\n",
    "df = pd.DataFrame(co.iter_chobjs(), columns = next(co.iter_chobjs()).keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '../utils/')\n",
    "from merge import combine\n",
    "\n",
    "df=df[(df['ins_tokens_str'].str.len() + df['del_tokens_str'].str.len()) <= 20]\n",
    "jlogie = pd.read_csv(\"../../John_Logie_Baird_FULL.csv\")\n",
    "merged = df.apply(lambda x: combine(x, jlogie), axis=1)\n",
    "\n",
    "# captures if we also want to use changeobjects that do not have tokens that are ground-truth labelled\n",
    "OUTER_JOIN = True\n",
    "\n",
    "merged = merged.dropna(how=\"all\")\n",
    "if not OUTER_JOIN:\n",
    "    merged = merged[(merged[\"birth_place\"].isna() & merged[\"Bulk\"].isna() & merged[\"nationality\"].isna() & merged[\"Link\"].isna())== False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def making_list(tokens):\n",
    "    new_tokens = []\n",
    "    for item in tokens:\n",
    "        new_tokens.append(' '.join(word for word in item if re.match(\"^[a-zA-Z0-9_]*$\", word)))\n",
    "\n",
    "    return(new_tokens)\n",
    "\n",
    "\n",
    "all_tokens = []\n",
    "merged = merged.reset_index()\n",
    "#merged=merged.drop(['level_0'], axis=1)\n",
    "for i, row in merged.iterrows():\n",
    "    all_tokens.append(making_list([row['left_token_str'],row['right_token_str']]))#,row['ins_tokens_str'],row['del_tokens_str']]))\n",
    "    merged.loc[i, 'left_token_str'] = all_tokens[i][0]\n",
    "    merged.loc[i,'right_token_str'] = all_tokens[i][1]\n",
    "    merged.loc[i,'ins_tokens_str'] = making_list([row['ins_tokens_str']])\n",
    "    merged.loc[i,'del_tokens_str'] = making_list([row['del_tokens_str']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DistilBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from pytorch_transformers import (DistilBertConfig,\n",
    "                                     DistilBertModel, \n",
    "                                     DistilBertTokenizer)\n",
    "from pytorch_transformers.modeling_utils import SequenceSummary\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased',\n",
    "                                    output_hidden_states=True,\n",
    "                                    output_attentions=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_summary = SequenceSummary(model.config)\n",
    "features = pd.DataFrame()\n",
    "new_vector = []\n",
    "for row in all_tokens:\n",
    "    vector=[]\n",
    "    for token in row:\n",
    "        if token.isspace() or token == '':\n",
    "            vector.append(np.full((768), 0))\n",
    "        else:\n",
    "            sen = torch.tensor([tokenizer.encode(token)])\n",
    "            vector.append(sequence_summary(model(sen)[0])[0].detach().numpy())\n",
    "    new_vector.append(np.concatenate((vector[0], vector[1])))\n",
    "    \n",
    "    #features = features.append(pd.DataFrame(np.concatenate((vector[0], vector[1]))).T)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(new_vector[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_summary = SequenceSummary(model.config)\n",
    "new_vector = []\n",
    "for row in all_tokens:\n",
    "    vector=[]\n",
    "    for token in row:\n",
    "        if token.isspace() or token == '':\n",
    "            vector.extend(np.full((768), 0))\n",
    "        else:\n",
    "            sen = torch.tensor([tokenizer.encode(token)])\n",
    "            vector.extend(sequence_summary(model(sen)[0])[0].detach().numpy())\n",
    "    #print(len(vector))\n",
    "    new_vector.append(vector)\n",
    "\n",
    "features = pd.DataFrame(np.row_stack(new_vector))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn import metrics\n",
    "db = DBSCAN(eps=2, min_samples=4).fit(features)\n",
    "labels = db.labels_\n",
    "\n",
    "# number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise_ = list(labels).count(-1)\n",
    "\n",
    "print('Estimated number of clusters: %d' % n_clusters_)\n",
    "print('Estimated number of noise points: %d' % n_noise_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "X = TSNE(random_state=42).fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_dbscan = pd.concat([pd.DataFrame(X), pd.Series(labels), merged], axis=1)\n",
    "\n",
    "plot_data_dbscan.columns = ['t-SNE-X', 't-SNE-Y', 'db_cluster', 'index','Bulk',\n",
    "                  'Link',          'action',     'birth_place',\n",
    "           'del_end_pos',   'del_start_pos',      'del_tokens',\n",
    "        'del_tokens_str',          'editor',        'from_rev',\n",
    "        'from_timestamp',     'ins_end_pos',   'ins_start_pos',\n",
    "            'ins_tokens',  'ins_tokens_str',      'left_neigh',\n",
    "            'left_token',  'left_token_str',     'nationality',\n",
    "               'page_id',     'right_neigh',     'right_token',\n",
    "       'right_token_str',            'text',          'to_rev',\n",
    "          'to_timestamp',           'token']\n",
    "plot_data_dbscan = plot_data_dbscan[plot_data_dbscan['db_cluster'] != -1].reset_index()\n",
    "plot_data_dbscan.to_csv(\"plotData_distilbert_dbscan_eps2_samples4.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
