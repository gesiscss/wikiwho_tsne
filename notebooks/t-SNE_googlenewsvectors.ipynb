{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"..\")\n",
    "\n",
    "import pandas as pd\n",
    "from wikiwho_chobj import Chobjer\n",
    "\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "co = Chobjer(article=\"39570\", pickles_path='pickles', lang='en', context=5)\n",
    "df = pd.DataFrame(co.iter_chobjs(), columns = next(co.iter_chobjs()).keys())\n",
    "\n",
    "jlogie = pd.read_csv(\"data/John_Logie_Baird_FULL.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge ground-truth labels with change object dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def combine(chobj):\n",
    "    # to be called by an apply function on a dataframe of change objects as provided by wikiwho\n",
    "    # depends on jlogie as ground truth labels\n",
    "    boolean = jlogie[\"rev_id\"] == chobj[\"to_rev\"]\n",
    "    token = jlogie[boolean]    \n",
    "    if not token.empty and len(token) == 1:\n",
    "        which_jlogie = token[\"token_id\"].isin(chobj[\"ins_tokens\"])\n",
    "        if np.sum(which_jlogie) == 1:\n",
    "            to_merge = jlogie.iloc[which_jlogie.index[0]]\n",
    "            chobj[\"nationality\"] = to_merge[\"nationality\"]\n",
    "            chobj[\"birth_place\"] = to_merge[\"birth_place\"]\n",
    "            chobj[\"Link\"] = to_merge[\"Link\"]\n",
    "            chobj[\"Bulk\"] = to_merge[\"Bulk\"]\n",
    "            chobj[\"token\"] = to_merge[\"token\"]\n",
    "            chobj[\"action\"] = to_merge[\"action\"]\n",
    "            return chobj\n",
    "        elif np.sum(which_jlogie) > 1:\n",
    "            print(\"more than one row in jlogie found!\")\n",
    "            return pd.Series(None)\n",
    "        elif np.sum(which_jlogie) == 0:\n",
    "            return pd.Series(None)\n",
    "    elif not token.empty and len(token) > 1:\n",
    "        which_jlogie = token[\"token_id\"].isin(chobj[\"ins_tokens\"])\n",
    "        if np.sum(which_jlogie) == 1:\n",
    "            to_merge = jlogie.iloc[which_jlogie.index[0]]\n",
    "            chobj[\"nationality\"] = to_merge[\"nationality\"]\n",
    "            chobj[\"birth_place\"] = to_merge[\"birth_place\"]\n",
    "            chobj[\"Link\"] = to_merge[\"Link\"]\n",
    "            chobj[\"Bulk\"] = to_merge[\"Bulk\"]\n",
    "            chobj[\"token\"] = to_merge[\"token\"]\n",
    "            chobj[\"action\"] = to_merge[\"action\"]\n",
    "            return chobj\n",
    "        elif np.sum(which_jlogie) == 0:\n",
    "            return pd.Series(None)\n",
    "        elif np.sum(which_jlogie) > 1:\n",
    "            for col in [\"nationality\", \"birth_place\", \"Link\", \"Bulk\"]:\n",
    "                if len(token[col].unique()) == 1:\n",
    "                    chobj[col] = list(token[col])[0]\n",
    "                else:\n",
    "                    chobj[col] = None\n",
    "                    print(\"non congruent values found for df['to_rev'] == \", str(chobj[\"to_rev\"]), \" and token ids: \", list(token[\"token_id\"]), \" in jlogie. Setting None to column \", str(col))\n",
    "            return chobj\n",
    "        return pd.Series(None)\n",
    "    else:\n",
    "        return pd.Series(None)\n",
    "\n",
    "pre_merge_optimization = df[df[\"to_rev\"].isin(jlogie[\"rev_id\"].unique())]\n",
    "merged = pre_merge_optimization.apply(lambda x: combine(x), axis=1)\n",
    "merged = merged.dropna(how=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Embed words by creating a vector of length 300 for each inserted and deleted tokens, so the resulting vector for one change object has length 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from gensim.sklearn_api import W2VTransformer\n",
    "from gensim.models import KeyedVectors\n",
    "from copy import deepcopy\n",
    "\n",
    "WORD_EMBED_SIZE = 300\n",
    "\n",
    "def transform(phrase : list, embedding):\n",
    "    li_vecs = []\n",
    "    for i in range(len(phrase)):\n",
    "        if phrase[i] in embedding:\n",
    "            li_vecs.append(deepcopy(embedding[phrase[i]]))\n",
    "    if len(li_vecs) != 0:\n",
    "        vecs = np.stack(li_vecs)\n",
    "        return vecs            \n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def filter_stopwords(phrase):\n",
    "    important_words = []\n",
    "    for word in phrase:\n",
    "        if word not in stopwords.words('english'):\n",
    "            important_words.append(word)\n",
    "    return important_words\n",
    "\n",
    "# Load vectors directly from the file\n",
    "embed = KeyedVectors.load_word2vec_format('data/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "def create_features(chobj):\n",
    "    ins_wordvecs = transform(filter_stopwords(list(chobj[\"ins_tokens_str\"])), embed)     \n",
    "    del_wordvecs = transform(filter_stopwords(list(chobj[\"del_tokens_str\"])), embed)\n",
    "    if ins_wordvecs is None:\n",
    "        ins_wordvecs = np.full(300, np.nan)\n",
    "    else:\n",
    "        ins_wordvecs = np.mean(ins_wordvecs, axis=0)\n",
    "    if del_wordvecs is None:\n",
    "        del_wordvecs = np.full(300, np.nan)\n",
    "    else:\n",
    "        del_wordvecs = np.mean(del_wordvecs, axis=0)\n",
    "\n",
    "    feat = pd.Series(np.nan_to_num(np.concatenate((ins_wordvecs, del_wordvecs))))\n",
    "\n",
    "    return feat\n",
    "\n",
    "Embedded = merged.apply(lambda x: create_features(x), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization and plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "X = TSNE().fit_transform(Embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_colors(entries):\n",
    "    col_list = []\n",
    "    \n",
    "    for ed in entries:\n",
    "        if ed == \"Y\":\n",
    "            col_list.append(\"r\")\n",
    "        if ed == \"N\":\n",
    "            col_list.append(\"b\")\n",
    "        if ed is None:\n",
    "            col_list.append(\"g\")\n",
    "            \n",
    "    return col_list\n",
    "            \n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### of birth place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18, 16), dpi= 80, facecolor='w', edgecolor='k')\n",
    "plt.scatter(X[:,0], X[:,1], c=convert_to_colors(merged[\"birth_place\"]),s=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### of nationality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18, 16), dpi= 80, facecolor='w', edgecolor='k')\n",
    "plt.scatter(X[:,0], X[:,1], c=convert_to_colors(merged[\"nationality\"]),s=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
