{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/home/heuzerothp/wikiwho_tsne\")\n",
    "\n",
    "import pandas as pd\n",
    "from wikiwho_chobj import Chobjer\n",
    "\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "\n",
    "co = Chobjer(article=\"39570\", pickles_path='pickles', lang='en', context=5)\n",
    "df = pd.DataFrame(co.iter_chobjs(), columns = next(co.iter_chobjs()).keys())\n",
    "\n",
    "\n",
    "jlogie = pd.read_csv(\"data/John_Logie_Baird_FULL.csv\")\n",
    "\n",
    "\n",
    "WORD_EMBED_SIZE = 300\n",
    "LEFT_CONTEXT = 5\n",
    "RIGHT_CONTEXT = 5\n",
    "GAP = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def combine(chobj):\n",
    "    # to be called by an apply function on a dataframe of change objects as provided by wikiwho\n",
    "    # depends on jlogie as ground truth labels\n",
    "    boolean = jlogie[\"rev_id\"] == chobj[\"to_rev\"]\n",
    "    token = jlogie[boolean]    \n",
    "    if not token.empty and len(token) == 1:\n",
    "        which_jlogie = token[\"token_id\"].isin(chobj[\"ins_tokens\"])\n",
    "        if np.sum(which_jlogie) == 1:\n",
    "            to_merge = jlogie.iloc[which_jlogie.index[0]]\n",
    "            chobj[\"nationality\"] = to_merge[\"nationality\"]\n",
    "            chobj[\"birth_place\"] = to_merge[\"birth_place\"]\n",
    "            chobj[\"Link\"] = to_merge[\"Link\"]\n",
    "            chobj[\"Bulk\"] = to_merge[\"Bulk\"]\n",
    "            chobj[\"token\"] = to_merge[\"token\"]\n",
    "            chobj[\"action\"] = to_merge[\"action\"]\n",
    "            return chobj\n",
    "        elif np.sum(which_jlogie) > 1:\n",
    "            print(\"more than one row in jlogie found!\")\n",
    "            return pd.Series(None)\n",
    "        elif np.sum(which_jlogie) == 0:\n",
    "            chobj[\"nationality\"] = None\n",
    "            chobj[\"birth_place\"] = None\n",
    "            chobj[\"Link\"] = None\n",
    "            chobj[\"Bulk\"] = None\n",
    "            chobj[\"token\"] = None\n",
    "            chobj[\"action\"] = None\n",
    "            return chobj\n",
    "    elif not token.empty and len(token) > 1:\n",
    "        which_jlogie = token[\"token_id\"].isin(chobj[\"ins_tokens\"])\n",
    "        if np.sum(which_jlogie) == 1:\n",
    "            to_merge = jlogie.iloc[which_jlogie.index[0]]\n",
    "            chobj[\"nationality\"] = to_merge[\"nationality\"]\n",
    "            chobj[\"birth_place\"] = to_merge[\"birth_place\"]\n",
    "            chobj[\"Link\"] = to_merge[\"Link\"]\n",
    "            chobj[\"Bulk\"] = to_merge[\"Bulk\"]\n",
    "            chobj[\"token\"] = to_merge[\"token\"]\n",
    "            chobj[\"action\"] = to_merge[\"action\"]\n",
    "            return chobj\n",
    "        elif np.sum(which_jlogie) == 0:\n",
    "            chobj[\"nationality\"] = None\n",
    "            chobj[\"birth_place\"] = None\n",
    "            chobj[\"Link\"] = None\n",
    "            chobj[\"Bulk\"] = None\n",
    "            chobj[\"token\"] = None\n",
    "            chobj[\"action\"] = None\n",
    "            return chobj\n",
    "        elif np.sum(which_jlogie) > 1:\n",
    "            for col in [\"nationality\", \"birth_place\", \"Link\", \"Bulk\"]:\n",
    "                if len(token[col].unique()) == 1:\n",
    "                    chobj[col] = list(token[col])[0]\n",
    "                else:\n",
    "                    chobj[col] = None\n",
    "                    print(\"non congruent values found for df['to_rev'] == \", str(chobj[\"to_rev\"]), \" and token ids: \", list(token[\"token_id\"]), \" in jlogie. Setting None to column \", str(col))\n",
    "            return chobj\n",
    "        return pd.Series(None)\n",
    "    else:\n",
    "        chobj[\"nationality\"] = None\n",
    "        chobj[\"birth_place\"] = None\n",
    "        chobj[\"Link\"] = None\n",
    "        chobj[\"Bulk\"] = None\n",
    "        chobj[\"token\"] = None\n",
    "        chobj[\"action\"] = None\n",
    "        return chobj\n",
    "\n",
    "merged = df.apply(lambda x: combine(x), axis=1)\n",
    "merged = merged.dropna(how=\"all\")\n",
    "merged = merged[(merged[\"birth_place\"].isna() & merged[\"Bulk\"].isna() & merged[\"nationality\"].isna() & merged[\"Link\"].isna())== False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove gaps longer than 10 tokens\n",
    "if GAP:\n",
    "    \n",
    "    merged = merged[merged[\"ins_tokens_str\"].apply(lambda x: len(x) <= 10) & merged[\"del_tokens_str\"].apply(lambda x: len(x) <= 10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Embed words by creating a vector of length 300 for each inserted and deleted tokens, so the resulting vector for one change object has length 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "def load_vectors(fname):\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    data = {}\n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        data[tokens[0]] = map(float, tokens[1:])\n",
    "    return data\n",
    "\n",
    "embed = load_vectors('data/wiki-news-300d-1M-subword.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from gensim.sklearn_api import W2VTransformer\n",
    "from gensim.models import KeyedVectors\n",
    "from copy import deepcopy\n",
    "import pdb\n",
    "\n",
    "def transform(phrase : list, embedding):\n",
    "    li_vecs = []\n",
    "    for i in range(len(phrase)):\n",
    "        if phrase[i] in embedding:\n",
    "            li_vecs.append(list(deepcopy(embedding[phrase[i]])))\n",
    "    if len(li_vecs) != 0:\n",
    "        vecs = np.stack(li_vecs)\n",
    "        return vecs            \n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def filter_stopwords(phrase):\n",
    "    important_words = []\n",
    "    for word in phrase:\n",
    "        if word not in stopwords.words('english'):\n",
    "            important_words.append(word)\n",
    "    return important_words\n",
    "\n",
    "def create_features(chobj, use_gap, left_context, right_context):\n",
    "    if left_context > 0:\n",
    "        left_wordvecs = transform(filter_stopwords(list(chobj[\"left_token_str\"][-left_context:])), embed)\n",
    "        if left_wordvecs is None:\n",
    "            left_wordvecs = np.full(WORD_EMBED_SIZE, 0)\n",
    "        else:\n",
    "            left_wordvecs = np.mean(left_wordvecs, axis=0)\n",
    "    if right_context > 0:\n",
    "        right_wordvecs = transform(filter_stopwords(list(chobj[\"right_token_str\"][:right_context])), embed)  \n",
    "        if right_wordvecs is None:\n",
    "            right_wordvecs = np.full(WORD_EMBED_SIZE, 0)\n",
    "        else:\n",
    "            right_wordvecs = np.mean(right_wordvecs, axis=0)\n",
    "    if use_gap:\n",
    "        ins_wordvecs = transform(filter_stopwords(list(chobj[\"ins_tokens_str\"])), embed)     \n",
    "        del_wordvecs = transform(filter_stopwords(list(chobj[\"del_tokens_str\"])), embed)\n",
    "        if ins_wordvecs is None:\n",
    "            ins_wordvecs = np.full(WORD_EMBED_SIZE, 0)\n",
    "        else:\n",
    "            ins_wordvecs = np.mean(ins_wordvecs, axis=0)\n",
    "        if del_wordvecs is None:\n",
    "            del_wordvecs = np.full(WORD_EMBED_SIZE, 0)\n",
    "        else:\n",
    "            del_wordvecs = np.mean(del_wordvecs, axis=0)\n",
    "    \n",
    "    li = []\n",
    "    for a in [\"left_wordvecs\", \"right_wordvecs\", \"ins_wordvecs\", \"del_wordvecs\"]:\n",
    "        if a in vars():\n",
    "            li.append(vars()[a])\n",
    "    \n",
    "    try:\n",
    "        feat = pd.Series(np.nan_to_num(np.concatenate(li)))\n",
    "    except ValueError:\n",
    "        pdb.set_trace()\n",
    "    \n",
    "    \n",
    "    return feat\n",
    "\n",
    "Embedded = merged.apply(lambda x: create_features(x, use_gap=GAP, left_context=LEFT_CONTEXT, right_context=RIGHT_CONTEXT), \n",
    "                        axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "clusterer = KMeans(random_state=42)\n",
    "clusters = clusterer.fit_predict(Embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(clusters).unique()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### find closest words to centroids for labeling clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "\n",
    "def average_vectors_to_shape_300(vec):\n",
    "    result = np.zeros(300)\n",
    "    fold = int(vec.shape[0]/300)\n",
    "    for i in range(300):\n",
    "        to_avg = []\n",
    "        for k in range(fold):\n",
    "            to_avg.append(vec[i + k *300])\n",
    "        result[i] = np.mean(to_avg)\n",
    "    return result\n",
    "\n",
    "centroids = clusterer.cluster_centers_\n",
    "centroids_300 = [np.zeros(300) for i in range(len(centroids))]\n",
    "for i in range(len(centroids)):\n",
    "    centroids_300[i] = average_vectors_to_shape_300(centroids[i])\n",
    "\n",
    "embed_keys = list(embed.keys())\n",
    "embed_vals = []\n",
    "for i in embed.values():\n",
    "    embed_vals.append(list(deepcopy(i)))\n",
    "    \n",
    "X = np.array(embed_vals)\n",
    "nbrs = NearestNeighbors(n_neighbors=5, algorithm='ball_tree').fit(X)\n",
    "dists, inds = nbrs.kneighbors(centroids_300)\n",
    "\n",
    "closest_words = {}\n",
    "for i in range(len(pd.Series(clusters).unique())):\n",
    "    print(i)\n",
    "    closest_words[i] = [(embed_keys[inds[i, j]], dists[i, j]) for j in range(len(inds[i]))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization and plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "X = TSNE(random_state=42).fit_transform(Embedded)\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def convert_editors_to_colors(editors):\n",
    "    ed_col = {}\n",
    "    col_list = []\n",
    "    for ed in editors:\n",
    "        if ed in ed_col:\n",
    "            col_list.append(ed_col[ed])\n",
    "        else:\n",
    "            r = lambda: random.randint(0,255)\n",
    "            new_col = '#%02X%02X%02X' % (r(),r(),r())\n",
    "            ed_col[ed] = new_col\n",
    "            col_list.append(new_col)\n",
    "            \n",
    "    return col_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18, 16), dpi= 80, facecolor='w', edgecolor='k')\n",
    "plt.scatter(X[:,0], X[:,1], s=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### interactively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "# enable javascript support\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "r = lambda: random.randint(0,255)\n",
    "\n",
    "traces = []\n",
    "for c in pd.Series(clusters).unique():\n",
    "    \n",
    "\n",
    "    # Create a trace\n",
    "    trace = go.Scatter(\n",
    "        x=X[clusters==c,0],\n",
    "        y=X[clusters==c,1],\n",
    "        mode = 'markers',\n",
    "        name = str(c),\n",
    "        marker = go.scatter.Marker(size=4, color='#%02X%02X%02X' % (r(),r(),r())),\n",
    "        text = Embedded.index,\n",
    "        showlegend = True,\n",
    "\n",
    "    )\n",
    "    traces.append(trace)\n",
    "\n",
    "data = traces\n",
    "\n",
    "# Plot and embed in ipython notebook\n",
    "iplot(data, filename='basic-scatter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "# enable javascript support\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "r = lambda: random.randint(0,255)\n",
    "\n",
    "traces = []\n",
    "for c in [\"Y\", \"N\", None]:\n",
    "    \n",
    "    if c is None:\n",
    "        trace = go.Scatter(\n",
    "            x=X[merged[\"nationality\"].isna(),0],\n",
    "            y=X[merged[\"nationality\"].isna(),1],\n",
    "            mode = 'markers',\n",
    "            name = str(c),\n",
    "            marker = go.scatter.Marker(size=4, color='#%02X%02X%02X' % (r(),r(),r())),\n",
    "            text = Embedded.index,\n",
    "            showlegend = True,\n",
    "        )\n",
    "    else:\n",
    "        # Create a trace\n",
    "        trace = go.Scatter(\n",
    "            x=X[merged[\"nationality\"]==c,0],\n",
    "            y=X[merged[\"nationality\"]==c,1],\n",
    "            mode = 'markers',\n",
    "            name = str(c),\n",
    "            marker = go.scatter.Marker(size=4, color='#%02X%02X%02X' % (r(),r(),r())),\n",
    "            text = Embedded.index,\n",
    "            showlegend = True,\n",
    "        )\n",
    "    traces.append(trace)\n",
    "\n",
    "data = traces\n",
    "\n",
    "# Plot and embed in ipython notebook\n",
    "iplot(data, filename='basic-scatter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Selection Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.graph_objs as go\n",
    "import plotly.offline as po\n",
    "import numpy as np\n",
    "from ipywidgets import interactive, HBox, VBox\n",
    "po.init_notebook_mode()\n",
    "import time\n",
    "import qgrid\n",
    "qgrid.set_grid_option('maxVisibleRows', 5)\n",
    "\n",
    "import pdb\n",
    "\n",
    "DISPLAYED_TABLE_COLUMNS = [\"t-SNE-X\", \"t-SNE-Y\"]\n",
    "if GAP:\n",
    "    DISPLAYED_TABLE_COLUMNS.append(\"ins_tokens_str\")\n",
    "    DISPLAYED_TABLE_COLUMNS.append(\"del_tokens_str\")\n",
    "if LEFT_CONTEXT:\n",
    "    DISPLAYED_TABLE_COLUMNS.append(\"left_token_str\")\n",
    "if RIGHT_CONTEXT:\n",
    "    DISPLAYED_TABLE_COLUMNS.append(\"right_token_str\")\n",
    "    \n",
    "if not (merged.index == range(len(merged))).all():\n",
    "    merged = merged.reset_index()\n",
    "    \n",
    "# TODO: NEEDS TO BE TESTED IF RIGHT ELEMENTS GET JOINED!!!    \n",
    "plot_data = pd.concat([pd.DataFrame(X), merged], axis=1)\n",
    "\n",
    "plot_data.columns = ['t-SNE-X',                 't-SNE-Y', 'index', 'Bulk',\n",
    "                  'Link',          'action',     'birth_place',\n",
    "           'del_end_pos',   'del_start_pos',      'del_tokens',\n",
    "        'del_tokens_str',          'editor',        'from_rev',\n",
    "        'from_timestamp',     'ins_end_pos',   'ins_start_pos',\n",
    "            'ins_tokens',  'ins_tokens_str',      'left_neigh',\n",
    "            'left_token',  'left_token_str',     'nationality',\n",
    "               'page_id',     'right_neigh',     'right_token',\n",
    "       'right_token_str',            'text',          'to_rev',\n",
    "          'to_timestamp',           'token']\n",
    "\n",
    "f = go.FigureWidget()\n",
    "scatter = f.add_scatter(x = plot_data[\"t-SNE-X\"], y = plot_data[\"t-SNE-Y\"], mode = 'markers')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# r = lambda: random.randint(0,255)\n",
    "\n",
    "# traces = []\n",
    "# for c in pd.Series(clusters).unique():\n",
    "#     # Create a trace\n",
    "#     trace = go.Scatter(\n",
    "#         x=plot_data.loc[clusters==c,\"t-SNE-X\"],\n",
    "#         y=plot_data.loc[clusters==c,\"t-SNE-Y\"],\n",
    "#         mode = 'markers',\n",
    "#         name = str(c),\n",
    "#         marker = go.scatter.Marker(size=4, color='#%02X%02X%02X' % (r(),r(),r())),\n",
    "#         text = Embedded.index,\n",
    "#         showlegend = True,\n",
    "\n",
    "#     )\n",
    "#     trace.on_selection(selection_fn)\n",
    "#     traces.append(trace)\n",
    "\n",
    "# f.data = traces\n",
    "\n",
    "\n",
    "f.layout.dragmode = 'lasso'\n",
    "\n",
    "N = len(plot_data)\n",
    "scatter.x = scatter.x + np.random.rand(N)/10 *(plot_data['t-SNE-X'].max() - plot_data['t-SNE-X'].min())\n",
    "scatter.y = scatter.y + np.random.rand(N)/10 *(plot_data['t-SNE-Y'].max() - plot_data['t-SNE-Y'].min())\n",
    "scatter.marker.opacity = 0.5\n",
    "\n",
    "# Create a table FigureWidget that updates on selection from points in the scatter plot of f\n",
    "t = go.FigureWidget([go.Table(  \n",
    "    header=dict(values=DISPLAYED_TABLE_COLUMNS,             \n",
    "                fill = dict(color='#C2D4FF'),\n",
    "                align = ['left'] * 5),\n",
    "    \n",
    "    cells=dict(values=[plot_data[col] for col in DISPLAYED_TABLE_COLUMNS],              \n",
    "               fill = dict(color='#F5F8FF'),\n",
    "               align = ['left'] * 5\n",
    "               ))])\n",
    "\n",
    "def selection_fn(trace,points,selector):   \n",
    "    with out:\n",
    "        clear_output()\n",
    "        #display(plot_data.loc[points.point_inds])\n",
    "        \n",
    "        display(qgrid.show_grid(plot_data.loc[points.point_inds, DISPLAYED_TABLE_COLUMNS]))\n",
    "    \n",
    "    t.data[0].cells.values = [plot_data.loc[points.point_inds][col] for col in DISPLAYED_TABLE_COLUMNS]\n",
    "scatter.on_selection(selection_fn)\n",
    "\n",
    "from IPython.display import display, clear_output\n",
    "display(f)\n",
    "\n",
    "from ipywidgets import widgets, Output\n",
    "out = Output()\n",
    "display(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
