{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merges change objects from wikiwho with our ground truth labels from the John Logie Baird Wikipedia article (section **Merging**). We could not use the normal join operator since we have to merge elements of the John Logie Baird table (**one token**) with elements from the change object table (**list of tokens**).\n",
    "\n",
    "Afterwards we calculate the features (section **Feature Creation**) using word embeddings, in this case [fasttext subword embeddings](https://fasttext.cc/docs/en/english-vectors.html) (for changing to other fasttext embeddings just replace the file (see dependencies), for google embeddings, there is code in the notebooks folder or just ask me;)). Here depending on the variables ``GAP``, ``LEFT_CONTEXT`` and ``RIGHT_CONTEXT`` (from `utils/const.py`) 300 to 1200 dimensions get created for each change object: `LEFT_CONTEXT` and `RIGHT_CONTEXT` respectively each account for 300 dimensions. Their values are integers so they state how many tokens of the left and right context are considered. `GAP` is a boolean variable that states if we include the gap, i.e. the deleted and inserted list of tokens. If `GAP` is set we have an additional 600 dimensions (300 for the inserted tokens and 300 for the deleted tokens).\n",
    "Since we have a list of tokens we *average* the values of the embeddings of inserted and deleted tokens and left and right context each.\n",
    "\n",
    "Then you have the choice of clustering the embeddings either with K-Means (section **KMEANS clustering**) or DBSCAN (section **DBSCAN clustering**). Note that the subsection **reverse look-up** performs a search of finding the closest words to the centroids of the clustering to label our clusters. This step is optional and actually nowhere needed in the code.\n",
    "\n",
    "Finally we reduce dimensionality of our data to plot it in a 2 dimensional graph using t-SNE (section **t-SNE**) and save the data to be executed by `notebooks/t-SNE_plotting.ipynb`.\n",
    "\n",
    "\n",
    "**Dependencies**:\n",
    "- `utils/const.py`: for constants shared with `notebooks/t-SNE_plotting.ipynb'\n",
    "- `utils/merge.py`: for merging of the two dataframes `jlogie` and `df`\n",
    "- `data/wiki-news-300d-1M-subword.vec`: the pre-trained word embeddings\n",
    "- locally installed [wikiwho wrapper](https://github.com/gesiscss/wikiwho_wrapper)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/home/heuzerothp/wikiwho_tsne\")\n",
    "\n",
    "import pandas as pd\n",
    "from wikiwho_chobj import Chobjer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from utils.vars import *\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "\n",
    "co = Chobjer(article=\"39570\", pickles_path='pickles', lang='en', context=max(LEFT_CONTEXT, RIGHT_CONTEXT))\n",
    "df = pd.DataFrame(co.iter_chobjs(), columns = next(co.iter_chobjs()).keys())\n",
    "\n",
    "jlogie = pd.read_csv(\"data/John_Logie_Baird_FULL.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from utils.merge import combine\n",
    "\n",
    "merged = df.apply(lambda x: combine(x, jlogie), axis=1)\n",
    "\n",
    "# captures if we also want to use change objects that do not match tokens that are ground-truth labelled\n",
    "OUTER_JOIN = False\n",
    "\n",
    "merged = merged.dropna(how=\"all\")\n",
    "if not OUTER_JOIN:\n",
    "    merged = merged[(merged[\"birth_place\"].isna() & merged[\"Bulk\"].isna() & merged[\"nationality\"].isna() & merged[\"Link\"].isna())== False]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### remove Bulks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove gaps longer than 20 tokens\n",
    "if GAP:\n",
    "\n",
    "    merged = merged[(merged['ins_tokens_str'].str.len() + merged['del_tokens_str'].str.len()) <= 20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Feature Creation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "def load_vectors(fname):\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    data = {}\n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        data[tokens[0]] = map(float, tokens[1:])\n",
    "    return data\n",
    "\n",
    "embed = load_vectors('data/wiki-news-300d-1M-subword.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from gensim.sklearn_api import W2VTransformer\n",
    "from gensim.models import KeyedVectors\n",
    "from copy import deepcopy\n",
    "import pdb\n",
    "\n",
    "def transform(phrase : list, embedding):\n",
    "    li_vecs = []\n",
    "    for i in range(len(phrase)):\n",
    "        \n",
    "        if phrase[i] in embedding:\n",
    "            li_vecs.append(list(deepcopy(embedding[phrase[i]])))\n",
    "    if len(li_vecs) != 0:\n",
    "        vecs = np.stack(li_vecs)\n",
    "        \n",
    "        return vecs            \n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def filter_stopwords(phrase):\n",
    "    important_words = []\n",
    "    for word in phrase:\n",
    "        if word not in stopwords.words('english'):\n",
    "            important_words.append(word)\n",
    "    return important_words\n",
    "\n",
    "def create_features(chobj, use_gap, left_context, right_context):\n",
    "    if left_context > 0:\n",
    "        left_wordvecs = transform(filter_stopwords(list(chobj[\"left_token_str\"][-left_context:])), embed)\n",
    "        if left_wordvecs is None:\n",
    "            left_wordvecs = np.full(WORD_EMBED_SIZE, 0)\n",
    "        else:\n",
    "            left_wordvecs = np.mean(left_wordvecs, axis=0)\n",
    "    if right_context > 0:\n",
    "        right_wordvecs = transform(filter_stopwords(list(chobj[\"right_token_str\"][:right_context])), embed)  \n",
    "        if right_wordvecs is None:\n",
    "            right_wordvecs = np.full(WORD_EMBED_SIZE, 0)\n",
    "        else:\n",
    "            right_wordvecs = np.mean(right_wordvecs, axis=0)\n",
    "    if use_gap:\n",
    "        ins_wordvecs = transform(filter_stopwords(list(chobj[\"ins_tokens_str\"])), embed)     \n",
    "        del_wordvecs = transform(filter_stopwords(list(chobj[\"del_tokens_str\"])), embed)\n",
    "        if ins_wordvecs is None:\n",
    "            ins_wordvecs = np.full(WORD_EMBED_SIZE, 0)\n",
    "        else:\n",
    "            ins_wordvecs = np.mean(ins_wordvecs, axis=0)\n",
    "        if del_wordvecs is None:\n",
    "            del_wordvecs = np.full(WORD_EMBED_SIZE, 0)\n",
    "        else:\n",
    "            del_wordvecs = np.mean(del_wordvecs, axis=0)\n",
    "    \n",
    "    li = []\n",
    "    for a in [\"left_wordvecs\", \"right_wordvecs\", \"ins_wordvecs\", \"del_wordvecs\"]:\n",
    "        if a in vars():\n",
    "            li.append(vars()[a])\n",
    "    \n",
    "    try:\n",
    "        feat = pd.Series(np.nan_to_num(np.concatenate(li)))\n",
    "    except ValueError:\n",
    "        pdb.set_trace()\n",
    "    \n",
    "    \n",
    "    return feat\n",
    "\n",
    "Embedded = merged.apply(lambda x: create_features(x, use_gap=GAP, left_context=LEFT_CONTEXT, right_context=RIGHT_CONTEXT), \n",
    "                        axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) K-means clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "clusterer = KMeans(random_state=42)\n",
    "clusters = clusterer.fit_predict(Embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(clusters).unique()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reverse look-up\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "\n",
    "def average_vectors_to_shape_300(vec):\n",
    "    result = np.zeros(300)\n",
    "    fold = int(vec.shape[0]/300)\n",
    "    for i in range(300):\n",
    "        to_avg = []\n",
    "        for k in range(fold):\n",
    "            to_avg.append(vec[i + k *300])\n",
    "        result[i] = np.mean(to_avg)\n",
    "    return result\n",
    "\n",
    "centroids = clusterer.cluster_centers_\n",
    "centroids_300 = [np.zeros(300) for i in range(len(centroids))]\n",
    "for i in range(len(centroids)):\n",
    "    centroids_300[i] = average_vectors_to_shape_300(centroids[i])\n",
    "\n",
    "embed_keys = list(embed.keys())\n",
    "embed_vals = []\n",
    "for i in embed.values():\n",
    "    embed_vals.append(list(deepcopy(i)))\n",
    "    \n",
    "X = np.array(embed_vals)\n",
    "nbrs = NearestNeighbors(n_neighbors=5, algorithm='ball_tree').fit(X)\n",
    "dists, inds = nbrs.kneighbors(centroids_300)\n",
    "\n",
    "closest_words = {}\n",
    "for i in range(len(pd.Series(clusters).unique())):\n",
    "    print(i)\n",
    "    closest_words[i] = [(embed_keys[inds[i, j]], dists[i, j]) for j in range(len(inds[i]))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) DBSCAN clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "clusterer = DBSCAN(eps=0.75, min_samples=5)\n",
    "clusters = clusterer.fit_predict(Embedded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(clusters).unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-SNE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "X = TSNE(random_state=42).fit_transform(Embedded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save data for plotting\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saves data for plotting\n",
    "\n",
    "if not (merged.index == range(len(merged))).all():\n",
    "    merged = merged.reset_index()\n",
    "    \n",
    "    \n",
    "plot_data = pd.concat([pd.DataFrame(X), pd.Series(clusters), merged], axis=1)\n",
    "\n",
    "\n",
    "plot_data.columns = ['t-SNE-X', 't-SNE-Y', 'cluster', 'Bulk',\n",
    "                     \n",
    "                  'Link',          'action',     'birth_place',\n",
    "           'del_end_pos',   'del_start_pos',      'del_tokens',\n",
    "        'del_tokens_str',          'editor',        'from_rev',\n",
    "        'from_timestamp',     'ins_end_pos',   'ins_start_pos',\n",
    "            'ins_tokens',  'ins_tokens_str',      'left_neigh',\n",
    "            'left_token',  'left_token_str',     'nationality',\n",
    "               'page_id',     'right_neigh',     'right_token',\n",
    "       'right_token_str',            'text',          'to_rev',\n",
    "          'to_timestamp',           'token']    \n",
    "\n",
    "\n",
    "plot_data.to_csv(\"data/plotData_jlb_inner_kmeans.csv\")\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
