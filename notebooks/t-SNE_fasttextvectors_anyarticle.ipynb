{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#os.chdir(\"/home/heuzerothp/wikiwho_tsne\")\n",
    "\n",
    "import pandas as pd\n",
    "from wikiwho_chobj import Chobjer\n",
    "\n",
    "import sys\n",
    "sys.path.append('../utils/')\n",
    "from _vars import *\n",
    "\n",
    "\n",
    "\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "LEFT_CONTEXT = 4\n",
    "RIGHT_CONTEXT = 4\n",
    "\n",
    "\n",
    "co = Chobjer(article=\"39570\", pickles_path= '../../bert', lang='en', context=max(LEFT_CONTEXT, RIGHT_CONTEXT))\n",
    "\n",
    "\n",
    "df = pd.DataFrame(co.iter_chobjs(), columns = next(co.iter_chobjs()).keys())\n",
    "df=df[(df['ins_tokens_str'].str.len() + df['del_tokens_str'].str.len()) <= 20]\n",
    "\n",
    "\n",
    "jlogie = pd.read_csv(\"../../John_Logie_Baird_FULL.csv\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from merge import combine\n",
    "\n",
    "merged = df.apply(lambda x: combine(x, jlogie), axis=1)\n",
    "\n",
    "# captures if we also want to use changeobjects that do not have tokens that are ground-truth labelled\n",
    "OUTER_JOIN = True\n",
    "\n",
    "merged = merged.dropna(how=\"all\")\n",
    "if not OUTER_JOIN:\n",
    "    merged = merged[(merged[\"birth_place\"].isna() & merged[\"Bulk\"].isna() & merged[\"nationality\"].isna() & merged[\"Link\"].isna())== False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merged.loc[92,'right_token_str'],'\\n', merged.loc[93,'right_token_str'],  '\\n',merged.loc[92,'left_token_str'],'\\n', merged.loc[93,'left_token_str'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merged.loc[92,'right_token'],'\\n', merged.loc[93,'right_token'],  '\\n',merged.loc[92,'left_token'],'\\n', merged.loc[93,'left_token'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### remove Bulks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove gaps longer than 20 tokens\n",
    "if GAP:\n",
    "    \n",
    "    \n",
    "    merged = merged[(merged['ins_tokens_str'].str.len() + merged['del_tokens_str'].str.len()) <= 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def making_list(tokens):\n",
    "    new_tokens = []\n",
    "    for item in tokens:\n",
    "        new_tokens.append(' '.join(word for word in item if re.match(\"^[a-zA-Z0-9_]*$\", word)))\n",
    "\n",
    "    return(new_tokens)\n",
    "\n",
    "\n",
    "all_tokens = []\n",
    "merged = merged.reset_index()\n",
    "#merged=merged.drop(['level_0'], axis=1)\n",
    "for i, row in merged.iterrows():\n",
    "    all_tokens.append(making_list([row['left_token_str'],row['right_token_str']]))#,row['ins_tokens_str'],row['del_tokens_str']]))\n",
    "    merged.loc[i, ['left_token_str','right_token_str']] = all_tokens[i]\n",
    "    #merged.loc[i,'right_token_str'] = all_tokens[i][1]\n",
    "    merged.loc[i,'ins_tokens_str'] = making_list([row['ins_tokens_str']])\n",
    "    merged.loc[i,'del_tokens_str'] = making_list([row['del_tokens_str']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Embed words by creating a vector of length 300 for each inserted and deleted tokens, so the resulting vector for one change object has length 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "def load_vectors(fname):\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    data = {}\n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        data[tokens[0]] = map(float, tokens[1:])\n",
    "    return data\n",
    "\n",
    "embed = load_vectors('../../wiki-news-300d-1M-subword.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from gensim.sklearn_api import W2VTransformer\n",
    "from gensim.models import KeyedVectors\n",
    "from copy import deepcopy\n",
    "import pdb\n",
    "\n",
    "def transform(phrase, embedding):\n",
    "    li_vecs = []\n",
    "    for i in range(len(phrase)):\n",
    "        if phrase[i] in embedding:\n",
    "            li_vecs.append(list(deepcopy(embedding[phrase[i]])))\n",
    "    if len(li_vecs) != 0:\n",
    "        vecs = np.stack(li_vecs)\n",
    "        \n",
    "        return vecs            \n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def filter_stopwords(phrase):\n",
    "    important_words = []\n",
    "    for word in phrase.split():\n",
    "        if word not in stopwords.words('english'):\n",
    "            important_words.append(word)\n",
    "    return important_words\n",
    "\n",
    "def create_features(chobj, use_gap, left_context, right_context):\n",
    "    if left_context > 0:\n",
    "        left_wordvecs = transform(filter_stopwords(chobj[\"left_token_str\"]), embed)\n",
    "        if left_wordvecs is None:\n",
    "            left_wordvecs = np.full(WORD_EMBED_SIZE, 0)\n",
    "        else:\n",
    "            left_wordvecs = np.mean(left_wordvecs, axis=0)\n",
    "    if right_context > 0:\n",
    "        right_wordvecs = transform(filter_stopwords(chobj[\"right_token_str\"]), embed)  \n",
    "        if right_wordvecs is None:\n",
    "            right_wordvecs = np.full(WORD_EMBED_SIZE, 0)\n",
    "        else:\n",
    "            right_wordvecs = np.mean(right_wordvecs, axis=0)\n",
    "    if use_gap:\n",
    "        ins_wordvecs = transform(filter_stopwords(chobj[\"ins_tokens_str\"]), embed)     \n",
    "        del_wordvecs = transform(filter_stopwords(chobj[\"del_tokens_str\"]), embed)\n",
    "        if ins_wordvecs is None:\n",
    "            ins_wordvecs = np.full(WORD_EMBED_SIZE, 0)\n",
    "        else:\n",
    "            ins_wordvecs = np.mean(ins_wordvecs, axis=0)\n",
    "        if del_wordvecs is None:\n",
    "            del_wordvecs = np.full(WORD_EMBED_SIZE, 0)\n",
    "        else:\n",
    "            del_wordvecs = np.mean(del_wordvecs, axis=0)\n",
    "    \n",
    "    li = []\n",
    "    for a in [\"left_wordvecs\", \"right_wordvecs\", \"ins_wordvecs\", \"del_wordvecs\"]:\n",
    "        if a in vars():\n",
    "            li.append(vars()[a])\n",
    "    \n",
    "    try:\n",
    "        feat = pd.Series(np.nan_to_num(np.concatenate(li)))\n",
    "    except ValueError:\n",
    "        pdb.set_trace()\n",
    "    \n",
    "    \n",
    "    return feat\n",
    "\n",
    "Embedded = merged.apply(lambda x: create_features(x, use_gap=GAP, left_context=LEFT_CONTEXT, right_context=RIGHT_CONTEXT), \n",
    "                        axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) K-means clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "clusterer = KMeans(random_state=42)\n",
    "clusters = clusterer.fit_predict(Embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(clusters).unique()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### find closest words to centroids for labeling clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "\n",
    "def average_vectors_to_shape_300(vec):\n",
    "    result = np.zeros(300)\n",
    "    fold = int(vec.shape[0]/300)\n",
    "    for i in range(300):\n",
    "        to_avg = []\n",
    "        for k in range(fold):\n",
    "            to_avg.append(vec[i + k *300])\n",
    "        result[i] = np.mean(to_avg)\n",
    "    return result\n",
    "\n",
    "centroids = clusterer.cluster_centers_\n",
    "centroids_300 = [np.zeros(300) for i in range(len(centroids))]\n",
    "for i in range(len(centroids)):\n",
    "    centroids_300[i] = average_vectors_to_shape_300(centroids[i])\n",
    "\n",
    "embed_keys = list(embed.keys())\n",
    "embed_vals = []\n",
    "for i in embed.values():\n",
    "    embed_vals.append(list(deepcopy(i)))\n",
    "    \n",
    "X = np.array(embed_vals)\n",
    "nbrs = NearestNeighbors(n_neighbors=5, algorithm='ball_tree').fit(X)\n",
    "dists, inds = nbrs.kneighbors(centroids_300)\n",
    "\n",
    "closest_words = {}\n",
    "for i in range(len(pd.Series(clusters).unique())):\n",
    "    print(i)\n",
    "    closest_words[i] = [(embed_keys[inds[i, j]], dists[i, j]) for j in range(len(inds[i]))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) DBSCAN clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "clusterer = DBSCAN(eps=0.5, min_samples=5)\n",
    "clusters = clusterer.fit_predict(Embedded)\n",
    "labels = clusterer.labels_\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise_ = list(labels).count(-1)\n",
    "\n",
    "print('Estimated number of clusters: %d' % n_clusters_)\n",
    "print('Estimated number of noise points: %d' % n_noise_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(clusters).unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-SNE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "X = TSNE(random_state=42).fit_transform(Embedded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save data for plotting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saves data for plotting\n",
    "\n",
    "if not (merged.index == range(len(merged))).all():\n",
    "    merged = merged.reset_index()\n",
    "    \n",
    "    \n",
    "plot_data = pd.concat([pd.DataFrame(X), pd.Series(clusters), merged], axis=1)\n",
    "\n",
    "plot_data.columns = ['t-SNE-X', 't-SNE-Y', 'cluster', 'index','Bulk', \n",
    "                     \n",
    "                  'Link',          'action',     'birth_place',\n",
    "           'del_end_pos',   'del_start_pos',      'del_tokens',\n",
    "        'del_tokens_str',          'editor',        'from_rev',\n",
    "        'from_timestamp',     'ins_end_pos',   'ins_start_pos',\n",
    "            'ins_tokens',  'ins_tokens_str',      'left_neigh',\n",
    "            'left_token',  'left_token_str',     'nationality',\n",
    "               'page_id',     'right_neigh',     'right_token',\n",
    "       'right_token_str',            'text',          'to_rev',\n",
    "          'to_timestamp',           'token']    \n",
    "\n",
    "\n",
    "plot_data.to_csv(\"../plotData_jlb_outer_dbscan_eps0_5.csv\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
